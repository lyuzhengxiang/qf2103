{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24605751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"matplotlib\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13bfbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_by_row(df, row_name, valid_strings):\n",
    "    \"\"\"\n",
    "    Drops columns from a DataFrame if the specified row (by name) does not contain any of the given valid strings.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to process.\n",
    "    row_name (str): The name of the row to check.\n",
    "    valid_strings (list): List of strings to check for.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame with columns removed.\n",
    "    \"\"\"\n",
    "    mask = df.loc[df.index[df.index.get_loc(row_name)]].astype(str).apply(lambda x: any(s in x for s in valid_strings))\n",
    "    return df.loc[:, mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a4a53fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>BA</th>\n",
       "      <th>CAT</th>\n",
       "      <th>CVX</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GS</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>JPM</th>\n",
       "      <th>KO</th>\n",
       "      <th>MCD</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NKE</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>PFE</th>\n",
       "      <th>SOFI</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>UNH</th>\n",
       "      <th>WMT</th>\n",
       "      <th>XOM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-23</th>\n",
       "      <td>0.006631</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>-0.016087</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>-0.002811</td>\n",
       "      <td>0.007167</td>\n",
       "      <td>-0.013538</td>\n",
       "      <td>-0.016508</td>\n",
       "      <td>-0.006606</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.006009</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>-0.021698</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.011195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-24</th>\n",
       "      <td>-0.003490</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.006385</td>\n",
       "      <td>0.019721</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>-0.003604</td>\n",
       "      <td>-0.005333</td>\n",
       "      <td>0.008896</td>\n",
       "      <td>-0.015830</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>-0.011250</td>\n",
       "      <td>0.024565</td>\n",
       "      <td>-0.003524</td>\n",
       "      <td>-0.024822</td>\n",
       "      <td>-0.006283</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>-0.014229</td>\n",
       "      <td>0.017113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-25</th>\n",
       "      <td>-0.001698</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>-0.058910</td>\n",
       "      <td>0.034123</td>\n",
       "      <td>0.024874</td>\n",
       "      <td>0.021094</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>-0.010809</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>-0.015890</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>-0.129258</td>\n",
       "      <td>-0.039404</td>\n",
       "      <td>0.014474</td>\n",
       "      <td>0.025084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-26</th>\n",
       "      <td>-0.009054</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.017627</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>-0.012913</td>\n",
       "      <td>-0.000376</td>\n",
       "      <td>-0.003824</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>-0.016795</td>\n",
       "      <td>-0.002325</td>\n",
       "      <td>0.019458</td>\n",
       "      <td>-0.009556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003929</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.019667</td>\n",
       "      <td>0.008743</td>\n",
       "      <td>0.008483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-29</th>\n",
       "      <td>-0.003592</td>\n",
       "      <td>0.013359</td>\n",
       "      <td>-0.001364</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>-0.000402</td>\n",
       "      <td>0.008636</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>0.023224</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.184070</td>\n",
       "      <td>0.041055</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.001261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-10</th>\n",
       "      <td>-0.024399</td>\n",
       "      <td>-0.014465</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>-0.028286</td>\n",
       "      <td>0.018719</td>\n",
       "      <td>-0.009897</td>\n",
       "      <td>-0.035126</td>\n",
       "      <td>-0.001477</td>\n",
       "      <td>-0.013499</td>\n",
       "      <td>-0.010425</td>\n",
       "      <td>-0.016128</td>\n",
       "      <td>-0.013302</td>\n",
       "      <td>-0.001263</td>\n",
       "      <td>-0.030435</td>\n",
       "      <td>-0.005226</td>\n",
       "      <td>-0.027876</td>\n",
       "      <td>-0.000507</td>\n",
       "      <td>-0.007329</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>-0.003654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-13</th>\n",
       "      <td>-0.010398</td>\n",
       "      <td>-0.002195</td>\n",
       "      <td>-0.008349</td>\n",
       "      <td>0.032238</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>-0.005378</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.016822</td>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>-0.004210</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>-0.019916</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>0.038523</td>\n",
       "      <td>-0.015933</td>\n",
       "      <td>0.025484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-14</th>\n",
       "      <td>-0.004790</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>-0.021032</td>\n",
       "      <td>0.024713</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>-0.007093</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.013261</td>\n",
       "      <td>0.006306</td>\n",
       "      <td>-0.007254</td>\n",
       "      <td>-0.003650</td>\n",
       "      <td>-0.012705</td>\n",
       "      <td>-0.011095</td>\n",
       "      <td>-0.014659</td>\n",
       "      <td>0.030104</td>\n",
       "      <td>-0.017383</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>-0.008118</td>\n",
       "      <td>0.003927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-15</th>\n",
       "      <td>0.019485</td>\n",
       "      <td>0.025347</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.030583</td>\n",
       "      <td>0.058431</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.019528</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>0.033436</td>\n",
       "      <td>-0.007220</td>\n",
       "      <td>0.067336</td>\n",
       "      <td>0.077314</td>\n",
       "      <td>-0.000589</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.016183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-16</th>\n",
       "      <td>-0.041239</td>\n",
       "      <td>-0.012117</td>\n",
       "      <td>0.016293</td>\n",
       "      <td>0.014985</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>-0.013592</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.019130</td>\n",
       "      <td>0.007580</td>\n",
       "      <td>0.007741</td>\n",
       "      <td>-0.009110</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>-0.019792</td>\n",
       "      <td>0.010245</td>\n",
       "      <td>0.049680</td>\n",
       "      <td>-0.034206</td>\n",
       "      <td>-0.062316</td>\n",
       "      <td>-0.000438</td>\n",
       "      <td>-0.001705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      AMZN        BA       CAT       CVX     GOOGL  \\\n",
       "Date                                                                     \n",
       "2024-01-23  0.006631  0.007979 -0.016087  0.000554 -0.002811  0.007167   \n",
       "2024-01-24 -0.003490  0.005433  0.012358  0.006385  0.019721  0.011226   \n",
       "2024-01-25 -0.001698  0.005594 -0.058910  0.034123  0.024874  0.021094   \n",
       "2024-01-26 -0.009054  0.008647  0.017627 -0.004465  0.003829  0.002105   \n",
       "2024-01-29 -0.003592  0.013359 -0.001364  0.012677 -0.000402  0.008636   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2025-01-10 -0.024399 -0.014465  0.001396 -0.028286  0.018719 -0.009897   \n",
       "2025-01-13 -0.010398 -0.002195 -0.008349  0.032238  0.014328 -0.005378   \n",
       "2025-01-14 -0.004790 -0.003209 -0.021032  0.024713  0.009928 -0.007093   \n",
       "2025-01-15  0.019485  0.025347 -0.004922  0.008895  0.009073  0.030583   \n",
       "2025-01-16 -0.041239 -0.012117  0.016293  0.014985  0.006610 -0.013592   \n",
       "\n",
       "                  GS       JNJ       JPM        KO       MCD      MSFT  \\\n",
       "Date                                                                     \n",
       "2024-01-23 -0.013538 -0.016508 -0.006606  0.004689  0.005481  0.006009   \n",
       "2024-01-24 -0.003604 -0.005333  0.008896 -0.015830  0.001299  0.009133   \n",
       "2024-01-25  0.008660  0.003767  0.014210  0.004235 -0.010809  0.005722   \n",
       "2024-01-26 -0.012913 -0.000376 -0.003824  0.003543 -0.016795 -0.002325   \n",
       "2024-01-29  0.007305 -0.000878  0.002609  0.006045  0.000171  0.014233   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2025-01-10 -0.035126 -0.001477 -0.013499 -0.010425 -0.016128 -0.013302   \n",
       "2025-01-13  0.005254  0.016822  0.017931  0.009453  0.004665 -0.004210   \n",
       "2025-01-14  0.015126  0.001936  0.013261  0.006306 -0.007254 -0.003650   \n",
       "2025-01-15  0.058431  0.001519  0.019528 -0.004362  0.002554  0.025275   \n",
       "2025-01-16  0.011601  0.019130  0.007580  0.007741 -0.009110 -0.004066   \n",
       "\n",
       "                 NKE      NVDA       PFE      SOFI      TSLA       UNH  \\\n",
       "Date                                                                     \n",
       "2024-01-23  0.013237  0.003664  0.004230 -0.021698  0.001627  0.004920   \n",
       "2024-01-24 -0.011250  0.024565 -0.003524 -0.024822 -0.006283 -0.004452   \n",
       "2024-01-25  0.000099  0.004147 -0.015890  0.011834 -0.129258 -0.039404   \n",
       "2024-01-26  0.019458 -0.009556  0.000000 -0.003929  0.003389  0.019667   \n",
       "2024-01-29  0.010937  0.023224  0.000364  0.184070  0.041055  0.002659   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2025-01-10 -0.001263 -0.030435 -0.005226 -0.027876 -0.000507 -0.007329   \n",
       "2025-01-13  0.012284 -0.019916  0.002990 -0.005670  0.021478  0.038523   \n",
       "2025-01-14 -0.012705 -0.011095 -0.014659  0.030104 -0.017383  0.004793   \n",
       "2025-01-15 -0.001406  0.033436 -0.007220  0.067336  0.077314 -0.000589   \n",
       "2025-01-16  0.000563 -0.019792  0.010245  0.049680 -0.034206 -0.062316   \n",
       "\n",
       "                 WMT       XOM  \n",
       "Date                            \n",
       "2024-01-23  0.002522  0.011195  \n",
       "2024-01-24 -0.014229  0.017113  \n",
       "2024-01-25  0.014474  0.025084  \n",
       "2024-01-26  0.008743  0.008483  \n",
       "2024-01-29  0.004676  0.001261  \n",
       "...              ...       ...  \n",
       "2025-01-10  0.012987 -0.003654  \n",
       "2025-01-13 -0.015933  0.025484  \n",
       "2025-01-14 -0.008118  0.003927  \n",
       "2025-01-15  0.006040  0.016183  \n",
       "2025-01-16 -0.000438 -0.001705  \n",
       "\n",
       "[248 rows x 20 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'Trading_Project_Data.csv'\n",
    "\n",
    "df = pd.read_csv(filename,\n",
    "                  index_col = 0,\n",
    "                    )\n",
    "\n",
    "\n",
    "df = df.drop('Date')\n",
    "df.index.name = 'Date'\n",
    "\n",
    "tickers = df.iloc[0]\n",
    "df.columns = [f\"{tickers[col]}_{col.split('.')[0]}\" for col in df.columns]\n",
    "df = df.drop('Ticker')\n",
    "\n",
    "df.index = pd.to_datetime(df.index, dayfirst=True)\n",
    "df = df.dropna()\n",
    "df.head()\n",
    "\n",
    "df = df.astype(float)\n",
    "\n",
    "data = df.loc[:, df.columns.str.contains('Close', case=False)].copy()\n",
    "data.columns = [col.split('_')[0] if ('Close' in col and '_' in col) else col for col in data.columns]\n",
    "data = np.log(data/data.shift(1)).dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba44cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_regression_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    # Convert start_date to datetime and find the index where it occurs\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "\n",
    "    # Determine the rolling window size as the length of rows before the start date\n",
    "    window = start_index  # This will be the number of rows before start_date\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        # Combine features and target\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "\n",
    "        for i in range(window, len(full_data)):\n",
    "            window_data = full_data.iloc[i - window:i]\n",
    "            X = window_data[[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = window_data[col]\n",
    "\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "\n",
    "            # Convert prediction to trading signal: 1 or -1\n",
    "            signal = 1 if pred > 0 else -1\n",
    "\n",
    "            # Multiply signal with actual log return to get predicted returns\n",
    "            predicted_returns.loc[pred_index, col] = signal * df.loc[pred_index, col]\n",
    "\n",
    "    # Filter by start_date\n",
    "    predicted_returns = predicted_returns[predicted_returns.index >= pd.to_datetime(start_date)]\n",
    "\n",
    "    return predicted_returns.astype(float)\n",
    "\n",
    "\n",
    "def rolling_DTclassification_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "    window = start_index  # Window size is the number of rows before start_date\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        # Combine features and target\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "\n",
    "        # Create the target: 1 if return is positive, -1 if return is negative\n",
    "        target = np.where(full_data[col] > 0, 1, -1)  # Binary target for classification\n",
    "\n",
    "        for i in range(window, len(full_data)):\n",
    "            window_data = full_data.iloc[i - window:i]\n",
    "            X = window_data[[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = target[i - window:i]  # Use binary target for classification\n",
    "\n",
    "            # Using DecisionTreeClassifier\n",
    "            model = DecisionTreeClassifier(max_depth = 5, random_state = 100)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "\n",
    "            # Multiply signal with actual log return to get predicted returns\n",
    "            predicted_returns.loc[pred_index, col] = pred * df.loc[pred_index, col]\n",
    "\n",
    "    # Filter by start_date\n",
    "    predicted_returns = predicted_returns[predicted_returns.index >= pd.to_datetime(start_date)]\n",
    "\n",
    "    return predicted_returns.astype(float)\n",
    "\n",
    "\n",
    "def rolling_MLPclassification_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "    window = start_index  # Window size is the number of rows before start_date\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        # Combine features and target\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "\n",
    "        # Create the target: 1 if return is positive, -1 if return is negative\n",
    "        target = np.where(full_data[col] > 0, 1, -1)  # Binary target for classification\n",
    "\n",
    "        for i in range(window, len(full_data)):\n",
    "            window_data = full_data.iloc[i - window:i]\n",
    "            X = window_data[[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = target[i - window:i]  # Use binary target for classification\n",
    "\n",
    "            # Using MLPClassifier\n",
    "            model = MLPClassifier(hidden_layer_sizes = 4 * [15], max_iter = 2000, random_state=100)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "\n",
    "            # Multiply signal with actual log return to get predicted returns\n",
    "            predicted_returns.loc[pred_index, col] = pred * df.loc[pred_index, col]\n",
    "\n",
    "    # Filter by start_date\n",
    "    predicted_returns = predicted_returns[predicted_returns.index >= pd.to_datetime(start_date)]\n",
    "\n",
    "    return predicted_returns.astype(float)\n",
    "\n",
    "\n",
    "def rolling_svc_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "    window = start_index  # Use all data before the start_date as training window\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        # Combine features and target\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "\n",
    "        # Create binary classification target\n",
    "        target = np.where(full_data[col] > 0, 1, -1)\n",
    "\n",
    "        for i in range(window, len(full_data)):\n",
    "            window_data = full_data.iloc[i - window:i]\n",
    "            X = window_data[[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = target[i - window:i]\n",
    "\n",
    "            model = SVC(kernel='linear')\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "\n",
    "            # Predicted return = signal * actual return\n",
    "            predicted_returns.loc[pred_index, col] = pred * df.loc[pred_index, col]\n",
    "\n",
    "    # Filter rows after the start_date\n",
    "    predicted_returns = predicted_returns[predicted_returns.index >= pd.to_datetime(start_date)]\n",
    "\n",
    "    return predicted_returns.astype(float)\n",
    "\n",
    "\n",
    "def rolling_logistic_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "    window = start_index  # Use all data before the start_date\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        # Combine features and target\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "\n",
    "        # Binary target: 1 for up, -1 for down\n",
    "        target = np.where(full_data[col] > 0, 1, -1)\n",
    "\n",
    "        for i in range(window, len(full_data)):\n",
    "            window_data = full_data.iloc[i - window:i]\n",
    "            X = window_data[[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = target[i - window:i]\n",
    "\n",
    "            # Logistic Regression model\n",
    "            model = LogisticRegression(random_state=100)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "\n",
    "            # Multiply predicted signal (1/-1) with actual log return\n",
    "            predicted_returns.loc[pred_index, col] = pred * df.loc[pred_index, col]\n",
    "\n",
    "    # Keep only rows after start_date\n",
    "    predicted_returns = predicted_returns[predicted_returns.index >= pd.to_datetime(start_date)]\n",
    "\n",
    "    return predicted_returns.astype(float)\n",
    "\n",
    "\n",
    "def benchmark_returns(df, start_date='2024-03-01'):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Convert start_date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "\n",
    "    # Filter the data from the start_date to the end of the dataset\n",
    "    filtered_data = df[df.index >= start_date]\n",
    "\n",
    "    # Sum the returns for each column from start_date to the end\n",
    "    column_sums = filtered_data.sum(axis=0)\n",
    "\n",
    "    # Apply np.exp to the sum of each column's returns\n",
    "    exp_column_sums = np.exp(column_sums)\n",
    "\n",
    "    return exp_column_sums\n",
    "\n",
    "\n",
    "def sum_and_exp_predicted_returns(predicted_returns):\n",
    "    # Sum each column of the predicted returns DataFrame\n",
    "    column_sums = predicted_returns.sum(axis=0)\n",
    "    \n",
    "    # Apply np.exp to the sum of each column\n",
    "    exp_column_sums = np.exp(column_sums)\n",
    "    \n",
    "    return exp_column_sums\n",
    "\n",
    "\n",
    "def highlight_max_row(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca9948",
   "metadata": {},
   "source": [
    "2 LAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fe0f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy time series data\n",
    "sd = '2024-03-01'\n",
    "lg = [1, 2]\n",
    "\n",
    "\n",
    "LR_preds = rolling_regression_predicted_returns(data,lg,sd)\n",
    "Log_preds = rolling_logistic_predicted_returns(data,lg,sd)\n",
    "DT_preds = rolling_DTclassification_predicted_returns(data,lg,sd)\n",
    "MLP_preds = rolling_MLPclassification_predicted_returns(data,lg,sd)\n",
    "SVC_preds = rolling_svc_predicted_returns(data,lg,sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "913bfb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = benchmark_returns(data)\n",
    "LR_preds_Sum = sum_and_exp_predicted_returns(LR_preds)\n",
    "Log_preds_Sum = sum_and_exp_predicted_returns(Log_preds)\n",
    "DT_preds_Sum = sum_and_exp_predicted_returns(DT_preds)\n",
    "MLP_preds_Sum = sum_and_exp_predicted_returns(MLP_preds)\n",
    "SVC_preds_Sum = sum_and_exp_predicted_returns(SVC_preds)\n",
    "\n",
    "\n",
    "# Merge both series into a DataFrame\n",
    "merged_df = pd.concat([benchmarks, LR_preds_Sum,Log_preds_Sum, DT_preds_Sum, MLP_preds_Sum, SVC_preds_Sum], axis=1)\n",
    "merged_df.columns = ['Benchmark', 'LinReg','Logistic', 'DecisionTree', 'DNN', 'SVC' ]\n",
    "\n",
    "# Apply styling\n",
    "styled_df_roll2 = merged_df.add_suffix('_Roll_2')\n",
    "# styled_df_roll2 = styled_df_roll2.style.apply(highlight_max_row, axis=1)\n",
    "# styled_df_roll2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c95dbc",
   "metadata": {},
   "source": [
    "3 LAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d08a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy time series data\n",
    "sd = '2024-03-01'\n",
    "lg = [1, 2, 3]\n",
    "\n",
    "\n",
    "LR_preds = rolling_regression_predicted_returns(data,lg,sd)\n",
    "Log_preds = rolling_logistic_predicted_returns(data,lg,sd)\n",
    "DT_preds = rolling_DTclassification_predicted_returns(data,lg,sd)\n",
    "MLP_preds = rolling_MLPclassification_predicted_returns(data,lg,sd)\n",
    "SVC_preds = rolling_svc_predicted_returns(data,lg,sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3248429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = benchmark_returns(data)\n",
    "LR_preds_Sum = sum_and_exp_predicted_returns(LR_preds)\n",
    "Log_preds_Sum = sum_and_exp_predicted_returns(Log_preds)\n",
    "DT_preds_Sum = sum_and_exp_predicted_returns(DT_preds)\n",
    "MLP_preds_Sum = sum_and_exp_predicted_returns(MLP_preds)\n",
    "SVC_preds_Sum = sum_and_exp_predicted_returns(SVC_preds)\n",
    "\n",
    "\n",
    "# Merge both series into a DataFrame\n",
    "merged_df = pd.concat([benchmarks, LR_preds_Sum,Log_preds_Sum, DT_preds_Sum, MLP_preds_Sum, SVC_preds_Sum], axis=1)\n",
    "merged_df.columns = ['Benchmark', 'LinReg','Logistic', 'DecisionTree', 'DNN', 'SVC' ]\n",
    "\n",
    "# Apply styling\n",
    "styled_df_roll3 = merged_df.add_suffix('_Roll_3')\n",
    "# styled_df_roll3 = styled_df_roll3.style.apply(highlight_max_row, axis=1)\n",
    "# styled_df_roll3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28285c0",
   "metadata": {},
   "source": [
    "Expanding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4b03a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_regression_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "\n",
    "        for i in range(start_index, len(full_data)):\n",
    "            X = full_data.iloc[:i][[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = full_data.iloc[:i][col]\n",
    "\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "            signal = 1 if pred > 0 else -1\n",
    "\n",
    "            predicted_returns.loc[pred_index, col] = signal * df.loc[pred_index, col]\n",
    "\n",
    "    return predicted_returns[predicted_returns.index >= start_date].astype(float)\n",
    "\n",
    "\n",
    "def expanding_DTclassification_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "        target = np.where(full_data[col] > 0, 1, -1)\n",
    "\n",
    "        for i in range(start_index, len(full_data)):\n",
    "            X = full_data.iloc[:i][[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = target[:i]\n",
    "\n",
    "            model = DecisionTreeClassifier(max_depth=5, random_state=100)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "\n",
    "            predicted_returns.loc[pred_index, col] = pred * df.loc[pred_index, col]\n",
    "\n",
    "    return predicted_returns[predicted_returns.index >= start_date].astype(float)\n",
    "\n",
    "\n",
    "def expanding_MLPclassification_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "        target = np.where(full_data[col] > 0, 1, -1)\n",
    "\n",
    "        for i in range(start_index, len(full_data)):\n",
    "            X = full_data.iloc[:i][[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = target[:i]\n",
    "\n",
    "            model = MLPClassifier(hidden_layer_sizes = 4 * [15], max_iter = 2000, random_state=100)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "\n",
    "            predicted_returns.loc[pred_index, col] = pred * df.loc[pred_index, col]\n",
    "\n",
    "    return predicted_returns[predicted_returns.index >= start_date].astype(float)\n",
    "\n",
    "\n",
    "def expanding_svc_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "        target = np.where(full_data[col] > 0, 1, -1)\n",
    "\n",
    "        for i in range(start_index, len(full_data)):\n",
    "            X = full_data.iloc[:i][[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = target[:i]\n",
    "\n",
    "            model = SVC(kernel='linear')\n",
    "            model.fit(X, y)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "\n",
    "            predicted_returns.loc[pred_index, col] = pred * df.loc[pred_index, col]\n",
    "\n",
    "    return predicted_returns[predicted_returns.index >= start_date].astype(float)\n",
    "\n",
    "\n",
    "def expanding_Logistic_predicted_returns(df, lags=[1, 2], start_date='2024-03-01'):\n",
    "    predicted_returns = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    start_index = df.index.get_indexer([start_date], method='ffill')[0]\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Create lagged features\n",
    "        lagged_data = pd.concat([series.shift(lag) for lag in lags], axis=1)\n",
    "        lagged_data.columns = [f'{col}_lag{lag}' for lag in lags]\n",
    "\n",
    "        full_data = pd.concat([series, lagged_data], axis=1).dropna()\n",
    "        target = np.where(full_data[col] > 0, 1, -1)\n",
    "\n",
    "        for i in range(start_index, len(full_data)):\n",
    "            X = full_data.iloc[:i][[f'{col}_lag{lag}' for lag in lags]]\n",
    "            y = target[:i]\n",
    "\n",
    "            model = LogisticRegression(random_state=100)\n",
    "            model.fit(X, y)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            current_features = full_data.iloc[i][[f'{col}_lag{lag}' for lag in lags]].to_frame().T\n",
    "            pred_index = full_data.index[i]\n",
    "            pred = model.predict(current_features)[0]\n",
    "\n",
    "            predicted_returns.loc[pred_index, col] = pred * df.loc[pred_index, col]\n",
    "\n",
    "    return predicted_returns[predicted_returns.index >= start_date].astype(float)\n",
    "\n",
    "\n",
    "def benchmark_returns(df, start_date='2024-03-01'):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Convert start_date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "\n",
    "    # Filter the data from the start_date to the end of the dataset\n",
    "    filtered_data = df[df.index >= start_date]\n",
    "\n",
    "    # Sum the returns for each column from start_date to the end\n",
    "    column_sums = filtered_data.sum(axis=0)\n",
    "\n",
    "    # Apply np.exp to the sum of each column's returns\n",
    "    exp_column_sums = np.exp(column_sums)\n",
    "\n",
    "    return exp_column_sums\n",
    "\n",
    "\n",
    "def sum_and_exp_predicted_returns(predicted_returns):\n",
    "    # Sum each column of the predicted returns DataFrame\n",
    "    column_sums = predicted_returns.sum(axis=0)\n",
    "    \n",
    "    # Apply np.exp to the sum of each column\n",
    "    exp_column_sums = np.exp(column_sums)\n",
    "    \n",
    "    return exp_column_sums\n",
    "\n",
    "\n",
    "def highlight_max_row(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b4d82",
   "metadata": {},
   "source": [
    "# 3 Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bad3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy time series data\n",
    "sd = '2024-03-01'\n",
    "lg = [1, 2, 3]\n",
    "\n",
    "\n",
    "LR_preds = expanding_regression_predicted_returns(data,lg,sd)\n",
    "Log_preds = expanding_Logistic_predicted_returns(data,lg,sd)\n",
    "DT_preds = expanding_DTclassification_predicted_returns(data,lg,sd)\n",
    "MLP_preds = expanding_MLPclassification_predicted_returns(data,lg,sd)\n",
    "SVC_preds = expanding_svc_predicted_returns(data,lg,sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ae366d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = benchmark_returns(data)\n",
    "LR_preds_Sum = sum_and_exp_predicted_returns(LR_preds)\n",
    "Log_preds_Sum = sum_and_exp_predicted_returns(Log_preds)\n",
    "DT_preds_Sum = sum_and_exp_predicted_returns(DT_preds)\n",
    "MLP_preds_Sum = sum_and_exp_predicted_returns(MLP_preds)\n",
    "SVC_preds_Sum = sum_and_exp_predicted_returns(SVC_preds)\n",
    "\n",
    "\n",
    "# Merge both series into a DataFrame\n",
    "merged_df = pd.concat([benchmarks, LR_preds_Sum,Log_preds_Sum, DT_preds_Sum, MLP_preds_Sum, SVC_preds_Sum], axis=1)\n",
    "merged_df.columns = ['Benchmark', 'LinReg','Logistic', 'DecisionTree', 'DNN', 'SVC' ]\n",
    "\n",
    "# Apply styling\n",
    "styled_df_exp3 = merged_df.add_suffix('_Exp_3')\n",
    "# styled_df_exp3 = styled_df_exp3.style.apply(highlight_max_row, axis=1)\n",
    "# styled_df_exp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d9e2c",
   "metadata": {},
   "source": [
    "# 2 Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7adda743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy time series data\n",
    "sd = '2024-03-01'\n",
    "lg = [1, 2]\n",
    "\n",
    "\n",
    "LR_preds = expanding_regression_predicted_returns(data,lg,sd)\n",
    "Log_preds = expanding_Logistic_predicted_returns(data,lg,sd)\n",
    "DT_preds = expanding_DTclassification_predicted_returns(data,lg,sd)\n",
    "MLP_preds = expanding_MLPclassification_predicted_returns(data,lg,sd)\n",
    "SVC_preds = expanding_svc_predicted_returns(data,lg,sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3c1ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = benchmark_returns(data)\n",
    "LR_preds_Sum = sum_and_exp_predicted_returns(LR_preds)\n",
    "Log_preds_Sum = sum_and_exp_predicted_returns(Log_preds)\n",
    "DT_preds_Sum = sum_and_exp_predicted_returns(DT_preds)\n",
    "MLP_preds_Sum = sum_and_exp_predicted_returns(MLP_preds)\n",
    "SVC_preds_Sum = sum_and_exp_predicted_returns(SVC_preds)\n",
    "\n",
    "\n",
    "# Merge both series into a DataFrame\n",
    "merged_df = pd.concat([benchmarks, LR_preds_Sum,Log_preds_Sum, DT_preds_Sum, MLP_preds_Sum, SVC_preds_Sum], axis=1)\n",
    "merged_df.columns = ['Benchmark', 'LinReg','Logistic', 'DecisionTree', 'DNN', 'SVC' ]\n",
    "\n",
    "# Apply styling\n",
    "styled_df_exp2 = merged_df.add_suffix('_Exp_2')\n",
    "# styled_df_exp2 = styled_df_exp2.style.apply(highlight_max_row, axis=1)\n",
    "# styled_df_exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7512e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d32c2_row0_col0, #T_d32c2_row1_col6, #T_d32c2_row2_col2, #T_d32c2_row3_col8, #T_d32c2_row4_col3, #T_d32c2_row5_col9, #T_d32c2_row5_col10, #T_d32c2_row5_col11, #T_d32c2_row6_col0, #T_d32c2_row7_col6, #T_d32c2_row8_col0, #T_d32c2_row9_col3, #T_d32c2_row10_col1, #T_d32c2_row11_col4, #T_d32c2_row12_col6, #T_d32c2_row13_col0, #T_d32c2_row14_col5, #T_d32c2_row15_col0, #T_d32c2_row16_col0, #T_d32c2_row17_col7, #T_d32c2_row18_col0, #T_d32c2_row19_col6 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d32c2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d32c2_level0_col0\" class=\"col_heading level0 col0\" >Benchmark_Roll_2</th>\n",
       "      <th id=\"T_d32c2_level0_col1\" class=\"col_heading level0 col1\" >LinReg_Roll_2</th>\n",
       "      <th id=\"T_d32c2_level0_col2\" class=\"col_heading level0 col2\" >DecisionTree_Roll_2</th>\n",
       "      <th id=\"T_d32c2_level0_col3\" class=\"col_heading level0 col3\" >DecisionTree_Roll_3</th>\n",
       "      <th id=\"T_d32c2_level0_col4\" class=\"col_heading level0 col4\" >LinReg_Exp_2</th>\n",
       "      <th id=\"T_d32c2_level0_col5\" class=\"col_heading level0 col5\" >Logistic_Exp_2</th>\n",
       "      <th id=\"T_d32c2_level0_col6\" class=\"col_heading level0 col6\" >DecisionTree_Exp_2</th>\n",
       "      <th id=\"T_d32c2_level0_col7\" class=\"col_heading level0 col7\" >SVC_Exp_2</th>\n",
       "      <th id=\"T_d32c2_level0_col8\" class=\"col_heading level0 col8\" >LinReg_Exp_3</th>\n",
       "      <th id=\"T_d32c2_level0_col9\" class=\"col_heading level0 col9\" >Logistic_Exp_3</th>\n",
       "      <th id=\"T_d32c2_level0_col10\" class=\"col_heading level0 col10\" >DNN_Exp_3</th>\n",
       "      <th id=\"T_d32c2_level0_col11\" class=\"col_heading level0 col11\" >SVC_Exp_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row0\" class=\"row_heading level0 row0\" >AAPL</th>\n",
       "      <td id=\"T_d32c2_row0_col0\" class=\"data row0 col0\" >1.267419</td>\n",
       "      <td id=\"T_d32c2_row0_col1\" class=\"data row0 col1\" >0.746606</td>\n",
       "      <td id=\"T_d32c2_row0_col2\" class=\"data row0 col2\" >1.006234</td>\n",
       "      <td id=\"T_d32c2_row0_col3\" class=\"data row0 col3\" >0.755891</td>\n",
       "      <td id=\"T_d32c2_row0_col4\" class=\"data row0 col4\" >0.749136</td>\n",
       "      <td id=\"T_d32c2_row0_col5\" class=\"data row0 col5\" >1.062599</td>\n",
       "      <td id=\"T_d32c2_row0_col6\" class=\"data row0 col6\" >0.828012</td>\n",
       "      <td id=\"T_d32c2_row0_col7\" class=\"data row0 col7\" >1.062599</td>\n",
       "      <td id=\"T_d32c2_row0_col8\" class=\"data row0 col8\" >0.703074</td>\n",
       "      <td id=\"T_d32c2_row0_col9\" class=\"data row0 col9\" >1.051584</td>\n",
       "      <td id=\"T_d32c2_row0_col10\" class=\"data row0 col10\" >1.043285</td>\n",
       "      <td id=\"T_d32c2_row0_col11\" class=\"data row0 col11\" >1.051584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row1\" class=\"row_heading level0 row1\" >AMZN</th>\n",
       "      <td id=\"T_d32c2_row1_col0\" class=\"data row1 col0\" >1.248359</td>\n",
       "      <td id=\"T_d32c2_row1_col1\" class=\"data row1 col1\" >1.182004</td>\n",
       "      <td id=\"T_d32c2_row1_col2\" class=\"data row1 col2\" >1.279027</td>\n",
       "      <td id=\"T_d32c2_row1_col3\" class=\"data row1 col3\" >1.303200</td>\n",
       "      <td id=\"T_d32c2_row1_col4\" class=\"data row1 col4\" >1.293421</td>\n",
       "      <td id=\"T_d32c2_row1_col5\" class=\"data row1 col5\" >1.229644</td>\n",
       "      <td id=\"T_d32c2_row1_col6\" class=\"data row1 col6\" >1.415348</td>\n",
       "      <td id=\"T_d32c2_row1_col7\" class=\"data row1 col7\" >1.225710</td>\n",
       "      <td id=\"T_d32c2_row1_col8\" class=\"data row1 col8\" >1.190438</td>\n",
       "      <td id=\"T_d32c2_row1_col9\" class=\"data row1 col9\" >1.253798</td>\n",
       "      <td id=\"T_d32c2_row1_col10\" class=\"data row1 col10\" >1.174724</td>\n",
       "      <td id=\"T_d32c2_row1_col11\" class=\"data row1 col11\" >1.261071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row2\" class=\"row_heading level0 row2\" >BA</th>\n",
       "      <td id=\"T_d32c2_row2_col0\" class=\"data row2 col0\" >0.829226</td>\n",
       "      <td id=\"T_d32c2_row2_col1\" class=\"data row2 col1\" >1.122572</td>\n",
       "      <td id=\"T_d32c2_row2_col2\" class=\"data row2 col2\" >1.690310</td>\n",
       "      <td id=\"T_d32c2_row2_col3\" class=\"data row2 col3\" >0.958306</td>\n",
       "      <td id=\"T_d32c2_row2_col4\" class=\"data row2 col4\" >1.006632</td>\n",
       "      <td id=\"T_d32c2_row2_col5\" class=\"data row2 col5\" >1.187119</td>\n",
       "      <td id=\"T_d32c2_row2_col6\" class=\"data row2 col6\" >1.052222</td>\n",
       "      <td id=\"T_d32c2_row2_col7\" class=\"data row2 col7\" >1.187119</td>\n",
       "      <td id=\"T_d32c2_row2_col8\" class=\"data row2 col8\" >1.423674</td>\n",
       "      <td id=\"T_d32c2_row2_col9\" class=\"data row2 col9\" >1.190671</td>\n",
       "      <td id=\"T_d32c2_row2_col10\" class=\"data row2 col10\" >1.190671</td>\n",
       "      <td id=\"T_d32c2_row2_col11\" class=\"data row2 col11\" >1.190671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row3\" class=\"row_heading level0 row3\" >CAT</th>\n",
       "      <td id=\"T_d32c2_row3_col0\" class=\"data row3 col0\" >1.152445</td>\n",
       "      <td id=\"T_d32c2_row3_col1\" class=\"data row3 col1\" >0.811741</td>\n",
       "      <td id=\"T_d32c2_row3_col2\" class=\"data row3 col2\" >0.960309</td>\n",
       "      <td id=\"T_d32c2_row3_col3\" class=\"data row3 col3\" >0.784252</td>\n",
       "      <td id=\"T_d32c2_row3_col4\" class=\"data row3 col4\" >0.879237</td>\n",
       "      <td id=\"T_d32c2_row3_col5\" class=\"data row3 col5\" >1.136485</td>\n",
       "      <td id=\"T_d32c2_row3_col6\" class=\"data row3 col6\" >1.279052</td>\n",
       "      <td id=\"T_d32c2_row3_col7\" class=\"data row3 col7\" >1.136485</td>\n",
       "      <td id=\"T_d32c2_row3_col8\" class=\"data row3 col8\" >1.752617</td>\n",
       "      <td id=\"T_d32c2_row3_col9\" class=\"data row3 col9\" >1.153931</td>\n",
       "      <td id=\"T_d32c2_row3_col10\" class=\"data row3 col10\" >1.174753</td>\n",
       "      <td id=\"T_d32c2_row3_col11\" class=\"data row3 col11\" >1.153931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row4\" class=\"row_heading level0 row4\" >CVX</th>\n",
       "      <td id=\"T_d32c2_row4_col0\" class=\"data row4 col0\" >1.081849</td>\n",
       "      <td id=\"T_d32c2_row4_col1\" class=\"data row4 col1\" >1.117579</td>\n",
       "      <td id=\"T_d32c2_row4_col2\" class=\"data row4 col2\" >1.136309</td>\n",
       "      <td id=\"T_d32c2_row4_col3\" class=\"data row4 col3\" >1.413412</td>\n",
       "      <td id=\"T_d32c2_row4_col4\" class=\"data row4 col4\" >1.023771</td>\n",
       "      <td id=\"T_d32c2_row4_col5\" class=\"data row4 col5\" >1.104593</td>\n",
       "      <td id=\"T_d32c2_row4_col6\" class=\"data row4 col6\" >0.855557</td>\n",
       "      <td id=\"T_d32c2_row4_col7\" class=\"data row4 col7\" >1.104593</td>\n",
       "      <td id=\"T_d32c2_row4_col8\" class=\"data row4 col8\" >0.868023</td>\n",
       "      <td id=\"T_d32c2_row4_col9\" class=\"data row4 col9\" >1.099350</td>\n",
       "      <td id=\"T_d32c2_row4_col10\" class=\"data row4 col10\" >1.099350</td>\n",
       "      <td id=\"T_d32c2_row4_col11\" class=\"data row4 col11\" >1.099350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row5\" class=\"row_heading level0 row5\" >GOOGL</th>\n",
       "      <td id=\"T_d32c2_row5_col0\" class=\"data row5 col0\" >1.398305</td>\n",
       "      <td id=\"T_d32c2_row5_col1\" class=\"data row5 col1\" >1.115356</td>\n",
       "      <td id=\"T_d32c2_row5_col2\" class=\"data row5 col2\" >1.093049</td>\n",
       "      <td id=\"T_d32c2_row5_col3\" class=\"data row5 col3\" >0.956674</td>\n",
       "      <td id=\"T_d32c2_row5_col4\" class=\"data row5 col4\" >1.108548</td>\n",
       "      <td id=\"T_d32c2_row5_col5\" class=\"data row5 col5\" >1.451888</td>\n",
       "      <td id=\"T_d32c2_row5_col6\" class=\"data row5 col6\" >1.123754</td>\n",
       "      <td id=\"T_d32c2_row5_col7\" class=\"data row5 col7\" >1.451888</td>\n",
       "      <td id=\"T_d32c2_row5_col8\" class=\"data row5 col8\" >1.012432</td>\n",
       "      <td id=\"T_d32c2_row5_col9\" class=\"data row5 col9\" >1.459330</td>\n",
       "      <td id=\"T_d32c2_row5_col10\" class=\"data row5 col10\" >1.459330</td>\n",
       "      <td id=\"T_d32c2_row5_col11\" class=\"data row5 col11\" >1.459330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row6\" class=\"row_heading level0 row6\" >GS</th>\n",
       "      <td id=\"T_d32c2_row6_col0\" class=\"data row6 col0\" >1.602419</td>\n",
       "      <td id=\"T_d32c2_row6_col1\" class=\"data row6 col1\" >0.690114</td>\n",
       "      <td id=\"T_d32c2_row6_col2\" class=\"data row6 col2\" >1.127970</td>\n",
       "      <td id=\"T_d32c2_row6_col3\" class=\"data row6 col3\" >1.234108</td>\n",
       "      <td id=\"T_d32c2_row6_col4\" class=\"data row6 col4\" >1.273678</td>\n",
       "      <td id=\"T_d32c2_row6_col5\" class=\"data row6 col5\" >1.223917</td>\n",
       "      <td id=\"T_d32c2_row6_col6\" class=\"data row6 col6\" >1.193853</td>\n",
       "      <td id=\"T_d32c2_row6_col7\" class=\"data row6 col7\" >1.233043</td>\n",
       "      <td id=\"T_d32c2_row6_col8\" class=\"data row6 col8\" >1.092228</td>\n",
       "      <td id=\"T_d32c2_row6_col9\" class=\"data row6 col9\" >1.225797</td>\n",
       "      <td id=\"T_d32c2_row6_col10\" class=\"data row6 col10\" >1.254821</td>\n",
       "      <td id=\"T_d32c2_row6_col11\" class=\"data row6 col11\" >1.225797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row7\" class=\"row_heading level0 row7\" >JNJ</th>\n",
       "      <td id=\"T_d32c2_row7_col0\" class=\"data row7 col0\" >0.937535</td>\n",
       "      <td id=\"T_d32c2_row7_col1\" class=\"data row7 col1\" >1.053884</td>\n",
       "      <td id=\"T_d32c2_row7_col2\" class=\"data row7 col2\" >0.914937</td>\n",
       "      <td id=\"T_d32c2_row7_col3\" class=\"data row7 col3\" >1.028413</td>\n",
       "      <td id=\"T_d32c2_row7_col4\" class=\"data row7 col4\" >0.902292</td>\n",
       "      <td id=\"T_d32c2_row7_col5\" class=\"data row7 col5\" >0.947930</td>\n",
       "      <td id=\"T_d32c2_row7_col6\" class=\"data row7 col6\" >1.111578</td>\n",
       "      <td id=\"T_d32c2_row7_col7\" class=\"data row7 col7\" >0.943521</td>\n",
       "      <td id=\"T_d32c2_row7_col8\" class=\"data row7 col8\" >0.813709</td>\n",
       "      <td id=\"T_d32c2_row7_col9\" class=\"data row7 col9\" >0.966056</td>\n",
       "      <td id=\"T_d32c2_row7_col10\" class=\"data row7 col10\" >0.973550</td>\n",
       "      <td id=\"T_d32c2_row7_col11\" class=\"data row7 col11\" >0.967154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row8\" class=\"row_heading level0 row8\" >JPM</th>\n",
       "      <td id=\"T_d32c2_row8_col0\" class=\"data row8 col0\" >1.397843</td>\n",
       "      <td id=\"T_d32c2_row8_col1\" class=\"data row8 col1\" >0.936844</td>\n",
       "      <td id=\"T_d32c2_row8_col2\" class=\"data row8 col2\" >0.940067</td>\n",
       "      <td id=\"T_d32c2_row8_col3\" class=\"data row8 col3\" >0.629883</td>\n",
       "      <td id=\"T_d32c2_row8_col4\" class=\"data row8 col4\" >1.361815</td>\n",
       "      <td id=\"T_d32c2_row8_col5\" class=\"data row8 col5\" >1.393200</td>\n",
       "      <td id=\"T_d32c2_row8_col6\" class=\"data row8 col6\" >0.977519</td>\n",
       "      <td id=\"T_d32c2_row8_col7\" class=\"data row8 col7\" >1.393200</td>\n",
       "      <td id=\"T_d32c2_row8_col8\" class=\"data row8 col8\" >0.916987</td>\n",
       "      <td id=\"T_d32c2_row8_col9\" class=\"data row8 col9\" >1.379383</td>\n",
       "      <td id=\"T_d32c2_row8_col10\" class=\"data row8 col10\" >1.376275</td>\n",
       "      <td id=\"T_d32c2_row8_col11\" class=\"data row8 col11\" >1.379383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row9\" class=\"row_heading level0 row9\" >KO</th>\n",
       "      <td id=\"T_d32c2_row9_col0\" class=\"data row9 col0\" >1.068831</td>\n",
       "      <td id=\"T_d32c2_row9_col1\" class=\"data row9 col1\" >0.941263</td>\n",
       "      <td id=\"T_d32c2_row9_col2\" class=\"data row9 col2\" >0.963284</td>\n",
       "      <td id=\"T_d32c2_row9_col3\" class=\"data row9 col3\" >1.156110</td>\n",
       "      <td id=\"T_d32c2_row9_col4\" class=\"data row9 col4\" >1.022510</td>\n",
       "      <td id=\"T_d32c2_row9_col5\" class=\"data row9 col5\" >1.021033</td>\n",
       "      <td id=\"T_d32c2_row9_col6\" class=\"data row9 col6\" >0.762521</td>\n",
       "      <td id=\"T_d32c2_row9_col7\" class=\"data row9 col7\" >1.018225</td>\n",
       "      <td id=\"T_d32c2_row9_col8\" class=\"data row9 col8\" >1.067770</td>\n",
       "      <td id=\"T_d32c2_row9_col9\" class=\"data row9 col9\" >1.007863</td>\n",
       "      <td id=\"T_d32c2_row9_col10\" class=\"data row9 col10\" >1.019200</td>\n",
       "      <td id=\"T_d32c2_row9_col11\" class=\"data row9 col11\" >1.007863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row10\" class=\"row_heading level0 row10\" >MCD</th>\n",
       "      <td id=\"T_d32c2_row10_col0\" class=\"data row10 col0\" >0.974744</td>\n",
       "      <td id=\"T_d32c2_row10_col1\" class=\"data row10 col1\" >1.332415</td>\n",
       "      <td id=\"T_d32c2_row10_col2\" class=\"data row10 col2\" >1.122719</td>\n",
       "      <td id=\"T_d32c2_row10_col3\" class=\"data row10 col3\" >1.088843</td>\n",
       "      <td id=\"T_d32c2_row10_col4\" class=\"data row10 col4\" >1.147234</td>\n",
       "      <td id=\"T_d32c2_row10_col5\" class=\"data row10 col5\" >0.789822</td>\n",
       "      <td id=\"T_d32c2_row10_col6\" class=\"data row10 col6\" >1.126869</td>\n",
       "      <td id=\"T_d32c2_row10_col7\" class=\"data row10 col7\" >0.786635</td>\n",
       "      <td id=\"T_d32c2_row10_col8\" class=\"data row10 col8\" >1.049407</td>\n",
       "      <td id=\"T_d32c2_row10_col9\" class=\"data row10 col9\" >0.822278</td>\n",
       "      <td id=\"T_d32c2_row10_col10\" class=\"data row10 col10\" >0.944761</td>\n",
       "      <td id=\"T_d32c2_row10_col11\" class=\"data row10 col11\" >0.822278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row11\" class=\"row_heading level0 row11\" >MSFT</th>\n",
       "      <td id=\"T_d32c2_row11_col0\" class=\"data row11 col0\" >1.032215</td>\n",
       "      <td id=\"T_d32c2_row11_col1\" class=\"data row11 col1\" >0.876364</td>\n",
       "      <td id=\"T_d32c2_row11_col2\" class=\"data row11 col2\" >0.809290</td>\n",
       "      <td id=\"T_d32c2_row11_col3\" class=\"data row11 col3\" >1.061803</td>\n",
       "      <td id=\"T_d32c2_row11_col4\" class=\"data row11 col4\" >1.271249</td>\n",
       "      <td id=\"T_d32c2_row11_col5\" class=\"data row11 col5\" >0.868580</td>\n",
       "      <td id=\"T_d32c2_row11_col6\" class=\"data row11 col6\" >1.221594</td>\n",
       "      <td id=\"T_d32c2_row11_col7\" class=\"data row11 col7\" >0.913053</td>\n",
       "      <td id=\"T_d32c2_row11_col8\" class=\"data row11 col8\" >1.133064</td>\n",
       "      <td id=\"T_d32c2_row11_col9\" class=\"data row11 col9\" >0.885760</td>\n",
       "      <td id=\"T_d32c2_row11_col10\" class=\"data row11 col10\" >0.849487</td>\n",
       "      <td id=\"T_d32c2_row11_col11\" class=\"data row11 col11\" >0.885760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row12\" class=\"row_heading level0 row12\" >NKE</th>\n",
       "      <td id=\"T_d32c2_row12_col0\" class=\"data row12 col0\" >0.695947</td>\n",
       "      <td id=\"T_d32c2_row12_col1\" class=\"data row12 col1\" >0.676790</td>\n",
       "      <td id=\"T_d32c2_row12_col2\" class=\"data row12 col2\" >0.719639</td>\n",
       "      <td id=\"T_d32c2_row12_col3\" class=\"data row12 col3\" >0.838669</td>\n",
       "      <td id=\"T_d32c2_row12_col4\" class=\"data row12 col4\" >1.015424</td>\n",
       "      <td id=\"T_d32c2_row12_col5\" class=\"data row12 col5\" >1.096561</td>\n",
       "      <td id=\"T_d32c2_row12_col6\" class=\"data row12 col6\" >1.344039</td>\n",
       "      <td id=\"T_d32c2_row12_col7\" class=\"data row12 col7\" >1.096561</td>\n",
       "      <td id=\"T_d32c2_row12_col8\" class=\"data row12 col8\" >0.797871</td>\n",
       "      <td id=\"T_d32c2_row12_col9\" class=\"data row12 col9\" >1.272135</td>\n",
       "      <td id=\"T_d32c2_row12_col10\" class=\"data row12 col10\" >1.244392</td>\n",
       "      <td id=\"T_d32c2_row12_col11\" class=\"data row12 col11\" >1.261930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row13\" class=\"row_heading level0 row13\" >NVDA</th>\n",
       "      <td id=\"T_d32c2_row13_col0\" class=\"data row13 col0\" >1.688845</td>\n",
       "      <td id=\"T_d32c2_row13_col1\" class=\"data row13 col1\" >1.438548</td>\n",
       "      <td id=\"T_d32c2_row13_col2\" class=\"data row13 col2\" >0.752545</td>\n",
       "      <td id=\"T_d32c2_row13_col3\" class=\"data row13 col3\" >1.306183</td>\n",
       "      <td id=\"T_d32c2_row13_col4\" class=\"data row13 col4\" >1.294433</td>\n",
       "      <td id=\"T_d32c2_row13_col5\" class=\"data row13 col5\" >1.567487</td>\n",
       "      <td id=\"T_d32c2_row13_col6\" class=\"data row13 col6\" >1.072956</td>\n",
       "      <td id=\"T_d32c2_row13_col7\" class=\"data row13 col7\" >1.567487</td>\n",
       "      <td id=\"T_d32c2_row13_col8\" class=\"data row13 col8\" >1.209648</td>\n",
       "      <td id=\"T_d32c2_row13_col9\" class=\"data row13 col9\" >1.554158</td>\n",
       "      <td id=\"T_d32c2_row13_col10\" class=\"data row13 col10\" >1.491295</td>\n",
       "      <td id=\"T_d32c2_row13_col11\" class=\"data row13 col11\" >1.554158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row14\" class=\"row_heading level0 row14\" >PFE</th>\n",
       "      <td id=\"T_d32c2_row14_col0\" class=\"data row14 col0\" >1.042641</td>\n",
       "      <td id=\"T_d32c2_row14_col1\" class=\"data row14 col1\" >0.731453</td>\n",
       "      <td id=\"T_d32c2_row14_col2\" class=\"data row14 col2\" >0.843249</td>\n",
       "      <td id=\"T_d32c2_row14_col3\" class=\"data row14 col3\" >0.857431</td>\n",
       "      <td id=\"T_d32c2_row14_col4\" class=\"data row14 col4\" >0.628611</td>\n",
       "      <td id=\"T_d32c2_row14_col5\" class=\"data row14 col5\" >1.080652</td>\n",
       "      <td id=\"T_d32c2_row14_col6\" class=\"data row14 col6\" >0.895906</td>\n",
       "      <td id=\"T_d32c2_row14_col7\" class=\"data row14 col7\" >1.074623</td>\n",
       "      <td id=\"T_d32c2_row14_col8\" class=\"data row14 col8\" >0.896834</td>\n",
       "      <td id=\"T_d32c2_row14_col9\" class=\"data row14 col9\" >0.900195</td>\n",
       "      <td id=\"T_d32c2_row14_col10\" class=\"data row14 col10\" >0.935862</td>\n",
       "      <td id=\"T_d32c2_row14_col11\" class=\"data row14 col11\" >0.930357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row15\" class=\"row_heading level0 row15\" >SOFI</th>\n",
       "      <td id=\"T_d32c2_row15_col0\" class=\"data row15 col0\" >1.815145</td>\n",
       "      <td id=\"T_d32c2_row15_col1\" class=\"data row15 col1\" >1.513566</td>\n",
       "      <td id=\"T_d32c2_row15_col2\" class=\"data row15 col2\" >0.618839</td>\n",
       "      <td id=\"T_d32c2_row15_col3\" class=\"data row15 col3\" >0.920236</td>\n",
       "      <td id=\"T_d32c2_row15_col4\" class=\"data row15 col4\" >1.115687</td>\n",
       "      <td id=\"T_d32c2_row15_col5\" class=\"data row15 col5\" >0.903398</td>\n",
       "      <td id=\"T_d32c2_row15_col6\" class=\"data row15 col6\" >0.841984</td>\n",
       "      <td id=\"T_d32c2_row15_col7\" class=\"data row15 col7\" >0.845111</td>\n",
       "      <td id=\"T_d32c2_row15_col8\" class=\"data row15 col8\" >0.742482</td>\n",
       "      <td id=\"T_d32c2_row15_col9\" class=\"data row15 col9\" >0.780861</td>\n",
       "      <td id=\"T_d32c2_row15_col10\" class=\"data row15 col10\" >0.875935</td>\n",
       "      <td id=\"T_d32c2_row15_col11\" class=\"data row15 col11\" >0.849376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row16\" class=\"row_heading level0 row16\" >TSLA</th>\n",
       "      <td id=\"T_d32c2_row16_col0\" class=\"data row16 col0\" >2.049832</td>\n",
       "      <td id=\"T_d32c2_row16_col1\" class=\"data row16 col1\" >0.425963</td>\n",
       "      <td id=\"T_d32c2_row16_col2\" class=\"data row16 col2\" >1.159205</td>\n",
       "      <td id=\"T_d32c2_row16_col3\" class=\"data row16 col3\" >1.039723</td>\n",
       "      <td id=\"T_d32c2_row16_col4\" class=\"data row16 col4\" >0.819811</td>\n",
       "      <td id=\"T_d32c2_row16_col5\" class=\"data row16 col5\" >0.723867</td>\n",
       "      <td id=\"T_d32c2_row16_col6\" class=\"data row16 col6\" >0.677097</td>\n",
       "      <td id=\"T_d32c2_row16_col7\" class=\"data row16 col7\" >0.810890</td>\n",
       "      <td id=\"T_d32c2_row16_col8\" class=\"data row16 col8\" >1.716061</td>\n",
       "      <td id=\"T_d32c2_row16_col9\" class=\"data row16 col9\" >0.588984</td>\n",
       "      <td id=\"T_d32c2_row16_col10\" class=\"data row16 col10\" >0.564930</td>\n",
       "      <td id=\"T_d32c2_row16_col11\" class=\"data row16 col11\" >0.668572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row17\" class=\"row_heading level0 row17\" >UNH</th>\n",
       "      <td id=\"T_d32c2_row17_col0\" class=\"data row17 col0\" >1.050616</td>\n",
       "      <td id=\"T_d32c2_row17_col1\" class=\"data row17 col1\" >0.787715</td>\n",
       "      <td id=\"T_d32c2_row17_col2\" class=\"data row17 col2\" >0.607405</td>\n",
       "      <td id=\"T_d32c2_row17_col3\" class=\"data row17 col3\" >0.740054</td>\n",
       "      <td id=\"T_d32c2_row17_col4\" class=\"data row17 col4\" >0.794958</td>\n",
       "      <td id=\"T_d32c2_row17_col5\" class=\"data row17 col5\" >1.101729</td>\n",
       "      <td id=\"T_d32c2_row17_col6\" class=\"data row17 col6\" >0.615618</td>\n",
       "      <td id=\"T_d32c2_row17_col7\" class=\"data row17 col7\" >1.102987</td>\n",
       "      <td id=\"T_d32c2_row17_col8\" class=\"data row17 col8\" >0.919561</td>\n",
       "      <td id=\"T_d32c2_row17_col9\" class=\"data row17 col9\" >0.941832</td>\n",
       "      <td id=\"T_d32c2_row17_col10\" class=\"data row17 col10\" >0.986198</td>\n",
       "      <td id=\"T_d32c2_row17_col11\" class=\"data row17 col11\" >0.925796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row18\" class=\"row_heading level0 row18\" >WMT</th>\n",
       "      <td id=\"T_d32c2_row18_col0\" class=\"data row18 col0\" >1.576426</td>\n",
       "      <td id=\"T_d32c2_row18_col1\" class=\"data row18 col1\" >1.168422</td>\n",
       "      <td id=\"T_d32c2_row18_col2\" class=\"data row18 col2\" >0.724511</td>\n",
       "      <td id=\"T_d32c2_row18_col3\" class=\"data row18 col3\" >0.763449</td>\n",
       "      <td id=\"T_d32c2_row18_col4\" class=\"data row18 col4\" >1.475753</td>\n",
       "      <td id=\"T_d32c2_row18_col5\" class=\"data row18 col5\" >1.558083</td>\n",
       "      <td id=\"T_d32c2_row18_col6\" class=\"data row18 col6\" >1.134445</td>\n",
       "      <td id=\"T_d32c2_row18_col7\" class=\"data row18 col7\" >1.558083</td>\n",
       "      <td id=\"T_d32c2_row18_col8\" class=\"data row18 col8\" >1.418198</td>\n",
       "      <td id=\"T_d32c2_row18_col9\" class=\"data row18 col9\" >1.538880</td>\n",
       "      <td id=\"T_d32c2_row18_col10\" class=\"data row18 col10\" >1.538880</td>\n",
       "      <td id=\"T_d32c2_row18_col11\" class=\"data row18 col11\" >1.538880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d32c2_level0_row19\" class=\"row_heading level0 row19\" >XOM</th>\n",
       "      <td id=\"T_d32c2_row19_col0\" class=\"data row19 col0\" >1.091248</td>\n",
       "      <td id=\"T_d32c2_row19_col1\" class=\"data row19 col1\" >0.798222</td>\n",
       "      <td id=\"T_d32c2_row19_col2\" class=\"data row19 col2\" >1.097737</td>\n",
       "      <td id=\"T_d32c2_row19_col3\" class=\"data row19 col3\" >0.938853</td>\n",
       "      <td id=\"T_d32c2_row19_col4\" class=\"data row19 col4\" >0.998634</td>\n",
       "      <td id=\"T_d32c2_row19_col5\" class=\"data row19 col5\" >0.994496</td>\n",
       "      <td id=\"T_d32c2_row19_col6\" class=\"data row19 col6\" >1.104870</td>\n",
       "      <td id=\"T_d32c2_row19_col7\" class=\"data row19 col7\" >0.994496</td>\n",
       "      <td id=\"T_d32c2_row19_col8\" class=\"data row19 col8\" >0.866782</td>\n",
       "      <td id=\"T_d32c2_row19_col9\" class=\"data row19 col9\" >0.960668</td>\n",
       "      <td id=\"T_d32c2_row19_col10\" class=\"data row19 col10\" >1.036722</td>\n",
       "      <td id=\"T_d32c2_row19_col11\" class=\"data row19 col11\" >0.960668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2437d411150>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate them side by side\n",
    "merged_df = pd.concat([styled_df_roll2, styled_df_roll3, styled_df_exp2, styled_df_exp3], axis=1)\n",
    "\n",
    "cols_to_drop = ['Benchmark_Roll_3', 'Benchmark_Exp_2', 'Benchmark_Exp_3']  # replace with your actual column names\n",
    "merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "mask = merged_df.eq(merged_df.max(axis=1), axis=0)\n",
    "\n",
    "# Drop columns that are never the max in any row\n",
    "cols_to_keep = mask.any(axis=0)\n",
    "merged_df = merged_df.loc[:, cols_to_keep]\n",
    "\n",
    "\n",
    "merged_df = merged_df.style.apply(highlight_max_row, axis=1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a28f1",
   "metadata": {},
   "source": [
    "NKE PFE BA UNH AMZN JNJ XOM CVX KO MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf7a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
